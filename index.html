<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0050)http://www.icst.pku.edu.cn/course/icb/MRS_MCI.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>MaskCLR</title>
<meta name="description" content="MaskCLR">
<meta name="author" content="MaskCLR">
<link rel="icon" type="image/x-icon" href="http://www.icst.pku.edu.cn/favicon.ico">
<link rel="stylesheet" type="text/css" href="files/project.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<script> 
	function coming_soon()
	{
		alert("We are cleaning up our code to make it more simple and readable");
	}
	</script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$']]},
        messageStyle: "none"
    });</script>

</head>


	
	

<body>
<div id="main">
  
	<div class="content"><br>
		<div class="title">
			<p class="banner" align="center"></p>
			<h1>MaskCLR: Attention-Guided Contrastive Learning for Robust Action Representation Learning </h1>
		</div>
		 <div class="description" style="font-size: 20px;">
			<p style="text-align: center;">
				<span style="margin-right: 20px;">Mohamed Abdelfattah</span>
				<span style="margin-right: 20px;">Mariam Hassan</span>
				<span>Alexandre Alahi</span>
				
				<br>
				<br>
				École Polytechnique Fédérale de Lausanne (EPFL)
				<br>
				firstname.lastname@epfl.ch
			</p>
		  </div>
		  
		<br><hr>
		<div class="teaser sec" style="text-align:center;">
			<img src="./files/act_vis2.png" alt="Teaser" width="100%">
			<p style="text-align: left;"><strong>Visualization of joint activations of MotionBERT and MaskCLR</strong>. The activated joints are more <span style="color:red">reddish</span> while the unactivated ones are more <span style="color:blue">blueish</span>. MaskCLR uses a bigger set of discriminative joints to recognize actions.</p>
		</div>
		

		<div class="abstract_sec">
		  <h2>Abstract</h2>
		  <div class="description">
			<p style="text-align: left;">
				Current transformer-based skeletal action recognition models tend to focus on a limited set of joints and low-level motion patterns to predict action classes. 
				This results in significant performance degradation under small skeleton perturbations or changing the pose estimator between training and testing. 
				In this work, we introduce MaskCLR, a new Masked Contrastive Learning approach for Robust skeletal action recognition. 
				We propose an Attention-Guided Probabilistic Masking (AGPM) strategy to occlude the most important joints and encourage the model to explore a larger set of discriminative joints. 
				Furthermore, we propose a Multi-Level Contrastive Learning (MLCL) paradigm to enforce the representations of standard and occluded skeletons to be class-discriminative, 
				i.e, more compact within each class and more dispersed across different classes. 
				Our approach helps the model capture the high-level action semantics instead of low-level joint variations, and can be conveniently incorporated into transformer-based models. 
				Without loss of generality, we combine MaskCLR with three transformer backbones: the vanilla transformer, DSTFormer, and STTFormer. 
				Extensive experiments on NTU60, NTU120, and Kinetics400 show that MaskCLR consistently outperforms previous state-of-the-art methods on standard and perturbed skeletons
				 from different pose estimators, showing improved accuracy, generalization, and robustness to skeleton perturbations.
			</p>
		  </div>
		</div>
		
		<div class="content has-text-justified">
			<div class="description">
				<p style="text-align: left;">
					Examples of model predictions under noisy skeletons. MaskCLR is more robust to noise, compared to MotionBERT using the same DSTFormer backbone.
				</p>
			</div>
			<img src="./files/robust_recog_003.gif" alt="Gif for Robust Recognition" height="50%">
		</div>
	
		<div class="Framework sec">
			<h2>Framework</h2>
			<img src="./files/overview_new.png" alt="Framework" width="100%">
			<p style="text-align: left;"> <strong>The pipeline of MaskCLR</strong>. Our approach consists of two (<strong>standard</strong> and <strong>masked</strong>) pathways that share the same weights. The standard pathway takes standard input skeletons while the masked pathway receives mostly the less activated joints from the standard pathway. Initially, the standard pathway is trained alone using the cross-entropy loss. The masked pathway, subsequently, comes into play to encourage the model to explore more discriminative joints. Using sample contrastive loss, we maximize the agreement of feature representations from the two pathways for the same skeleton sequence and vice versa. Additionally, to exploit the high semantic consistency between same-class skeleton sequences, we maximize the similarity between the class-wise average representations from the two pathways using class contrastive loss. Ultimately, the two contrastive losses contribute to the formation of a disentangled feature space, effectively improving the accuracy, robustness, and generalization of the model. At test time, only the standard pathway is used.</p>
			</div>

		<div class="experiments_sec">
			<h2>Results</h2>
			<div class="picture_wrapper" width="100%">
				<img src="./files/table1.png" alt="Results" width="100%">
				</br>
				<img src="./files/plots.png" alt="Results" width="100%">
				</br>	
				<img src="./files/feature_space.png" alt="Results" width="100%">
			</div>
		</div>
		  
		
	  	<div class="download_sec">
			<h2>Resources</h2>
			<div>
				<!-- <li><strong>Paper</strong>:  <a href="https://openreview.net/pdf?id=PVpfUEF3ze">OpenReview</a></li> -->
				<li><strong>Supplementary material</strong>: <a href="https://www.dropbox.com/scl/fi/z5i19n9fkr89q1i29xlei/supplementary.pdf?rlkey=o6tdxlgp44p31af93qx3gnfi9&dl=0">Dropbox link</a></li>
				<!-- <li><strong>Code</strong>:  Coming soon</li> -->
				<!-- <a href="https://anonymous.4open.science/r/MaskCLR-A503">Coming soon</a> -->

			</div>
		</div>
		
		
       <div>
          <h2>Reference</h2>
            <p>[1]. Zhu Wentao, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne Wu, and Yizhou Wang. "MotionBERT: A Unified Perspective on Learning Human Motion Representations." In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.</p>
			<p>[2]. Duan Haodong, Yue Zhao, Kai Chen, Dahua Lin, and Bo Dai. "Revisiting skeleton-based action recognition." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.</p>
			<p>[3]. Shahroudy Amir, Jun Liu, Tian-Tsong Ng, and Gang Wang. "Ntu rgb+ d: A large scale dataset for 3d human activity analysis." In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.</p>
        </div>
	
  </div>
</div>


</body></html>
